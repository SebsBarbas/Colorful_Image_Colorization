{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "!kill $(ps aux | grep './ngrok' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from Colorization import *\n",
    "from LoadDataset2 import *\n",
    "from tensorboard import notebook\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "#config = tf.compat.v1.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(2)\n",
    "tf.print(a)\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "   print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from Colorization import *\n",
    "import glob\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from scipy import spatial\n",
    "import datetime\n",
    "\n",
    "files = \"./test2014/*.jpg\"\n",
    "imges = [f for f in glob.glob(files)]\n",
    "\n",
    "\n",
    "def load_data(ab_2_class, ab_points):\n",
    "    train_images = []\n",
    "    out_images = []\n",
    "    for file in imges:\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(file), cv2.COLOR_BGR2LAB)\n",
    "\n",
    "        if len(train_images) == 0:\n",
    "            test = cv2.resize(img, (256, 256))\n",
    "            train_images = [(cv2.resize(img, (256, 256))[:, :, 0] / 255).reshape((256, 256, 1))]\n",
    "        else:\n",
    "            train_images.append((cv2.resize(img, (256, 256))[:, :, 0] / 255).reshape(256, 256, 1))\n",
    "\n",
    "        output = cv2.resize(img, (64, 64))[:, :, 1:]\n",
    "        #out_im = np.zeros((output.shape[0], output.shape[1], 313))\n",
    "        out_im = np.zeros((output.shape[0], output.shape[1], 1))\n",
    "\n",
    "        for i in range(out_im.shape[0]):\n",
    "            for j in range(out_im.shape[1]):\n",
    "                ab_val = output[i, j] // 10\n",
    "                ab_val = ab_val * 10\n",
    "                if str([ab_val[0], ab_val[1]]) not in ab_2_class.keys():\n",
    "                    key =  ab_points[spatial.KDTree(ab_points).query(ab_val)[1]]\n",
    "                    ab_val = key\n",
    "                pos = ab_2_class[str([int(ab_val[0]), int(ab_val[1])])]\n",
    "                #out_im[i, j, pos] = 1\n",
    "                out_im[i, j, 0] = pos\n",
    "\n",
    "        if len(out_images) == 0:\n",
    "            out_images = [out_im]\n",
    "        else:\n",
    "            out_images.append(out_im)\n",
    "\n",
    "    train = train_images[128:]\n",
    "    train_out = out_images[128:]\n",
    "    test = train_images[:128]\n",
    "    test_out = out_images[:128]\n",
    "\n",
    "    return train, train_out, test, test_out\n",
    "\n",
    "def decode(ab_image, maps, T=0.35):\n",
    "\n",
    "    ab_second = tf.multiply(ab_image, 1/T)\n",
    "    ab_second = tf.keras.activations.softmax(ab_second, -1)\n",
    "\n",
    "    max_pos_soft = tf.math.argmax(ab_second, -1)\n",
    "    max_pos = tf.math.argmax(ab_image, -1)\n",
    "    shapes = tf.shape(max_pos)\n",
    "    shapes = shapes.numpy()\n",
    "\n",
    "    decoding_soft = np.zeros((shapes[0], shapes[1], 2))\n",
    "    decoding = np.zeros((shapes[0], shapes[1], 2))\n",
    "\n",
    "    for i in range(shapes[0]):\n",
    "        for j in range(shapes[1]):\n",
    "            val_soft = tf.gather_nd(max_pos_soft, (i, j)).numpy()\n",
    "            tup_soft = maps[str(val_soft)]\n",
    "            decoding_soft[i, j, 0] = tup_soft[0]\n",
    "            decoding_soft[i, j, 1] = tup_soft[1]\n",
    "\n",
    "            val = tf.gather_nd(max_pos, (i, j)).numpy()\n",
    "            tup = maps[str(val)]\n",
    "            decoding[i, j, 0] = tup[0]\n",
    "            decoding[i, j, 1] = tup[1]\n",
    "\n",
    "\n",
    "\n",
    "    return decoding, decoding_soft\n",
    "\n",
    "cat_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "spars_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def loss_color(conv_out, gt_ab):\n",
    "\n",
    "    flat_conv_out = tf.reshape(conv_out, [-1, 313])\n",
    "    flat_gt_ab = tf.reshape(gt_ab, [-1, 1])\n",
    "    #loss = cat_loss(flat_gt_ab, flat_conv_out)/conv_out.get_shape()[0]\n",
    "    loss = spars_loss(flat_gt_ab, flat_conv_out) / conv_out.get_shape()[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training = True)\n",
    "        print(predictions.get_shape())\n",
    "        print(labels.get_shape())\n",
    "        print(\"Flat predictions \")\n",
    "        flat_conv_out = tf.reshape(predictions, [-1, 313])\n",
    "        print(flat_conv_out.get_shape())\n",
    "        print(\"Flat labels \")\n",
    "        flat_gt_ab = tf.reshape(labels, [-1, 1])\n",
    "        print(flat_gt_ab.get_shape())\n",
    "        loss =loss_color(predictions, labels)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training = False)\n",
    "    t_loss = loss_color(predictions, labels)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "model = Colorization((256, 256, 1))\n",
    "batch_size = 128\n",
    "train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'test_accuracy')\n",
    "test_top1 = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1, name = 'test_1_accuracy')\n",
    "test_top5 = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name = 'test_5_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_model_dir = \".\\checkpoints\"\n",
    "    infile = open('dictionaries.p', 'rb')\n",
    "    list_dict = pickle.load(infile)\n",
    "    infile.close()\n",
    "    map_ab_2_class = list_dict[0]\n",
    "    map_class_2_ab = list_dict[1]\n",
    "    ab_points = list_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    train, train_out, test, test_out = load_data(map_ab_2_class, ab_points)\n",
    "\n",
    "    data = [train, train_out, test, test_out]\n",
    "\n",
    "    with open('data.p', 'wb') as fp:\n",
    "        pickle.dump(data, fp, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    infile = open('data.p', 'rb')\n",
    "    list_data = pickle.load(infile)\n",
    "    infile.close()\n",
    "\n",
    "    [train, train_out, test, test_out] = list_data\n",
    "\n",
    "    train_set = tf.data.Dataset.from_tensor_slices((train, train_out)).batch(batch_size)\n",
    "    test_set = tf.data.Dataset.from_tensor_slices((test, test_out)).batch(batch_size)\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    checkpoint_dir = os.path.join(save_model_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(optimizer = optimizer, model = model)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep = 3)\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restaurado de {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Inicializando desde cero\")\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "    test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-caf88ada333e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip -o ngrok-stable-linux-amd64.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;31m# Ensure new system_piped implementation is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pexpect-U\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'<pexpect factory incomplete>'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 304\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptyproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_spawnpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_native_pty_fork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;31m# Use internal fork_pty, for Solaris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pty.py\u001b[0m in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmaster_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslave_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHILD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Establish a new session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e4922efd389e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m get_ipython().system_raw(\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./ngrok http 6006 &'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_raw\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2490\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m                 \u001b[0;31m# Use env shell instead of default /bin/sh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2492\u001b[0;31m                 \u001b[0mec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2493\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2494\u001b[0m                 \u001b[0;31m# intercept control-C; a long traceback is not useful here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    798\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1480\u001b[0m                             \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m                             \u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrpipe_write\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                             restore_signals, start_new_session, preexec_fn)\n\u001b[0m\u001b[1;32m   1483\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_child_created\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "# Run 2 times\n",
    "logdir = \"logs\"\n",
    "LOG_DIR = logdir\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i value is \n",
      "0\n",
      "WARNING:tensorflow:Layer colorization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "(128, 64, 64, 313)\n",
      "(128, 64, 64, 1)\n",
      "Flat predictions \n",
      "(524288, 313)\n",
      "Flat labels \n",
      "(524288, 1)\n",
      "(128, 64, 64, 313)\n",
      "(128, 64, 64, 1)\n",
      "Flat predictions \n",
      "(524288, 313)\n",
      "Flat labels \n",
      "(524288, 1)\n",
      "(115, 64, 64, 313)\n",
      "(115, 64, 64, 1)\n",
      "Flat predictions \n",
      "(471040, 313)\n",
      "Flat labels \n",
      "(471040, 1)\n"
     ]
    }
   ],
   "source": [
    "    for i in range(100):\n",
    "        print(\"i value is \")\n",
    "        print(i)\n",
    "        \n",
    "        for images, labels in train_set:\n",
    "            train_step(images, labels)\n",
    "                \n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=i+1)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=i+1)\n",
    "\n",
    "\n",
    "        \n",
    "        for img, lbl in test_set:\n",
    "            test_step(img, lbl)\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', test_loss.result(), step = i + 1)\n",
    "            tf.summary.scalar('accuracy', test_accuracy.result(), step = i + 1)\n",
    "\n",
    "        save_path = manager.save()\n",
    "\n",
    "        template = 'Epoch {}, Train Loss: {}, Train Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print(template.format(i+1, train_loss.result(), train_accuracy.result()*100, \\\n",
    "                                test_loss.result(), test_accuracy.result()*100))\n",
    "        template = 'Top1 Error: {}, Top5 Error: {}'\n",
    "        print(template.format((1 - test_top1.result())*100,\\\n",
    "                                    (1 - test_top5.result())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='Colorization_net_architecture.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv1_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [256, 256, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-58fd8dfa14d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    680\u001b[0m                            'method accepts an `inputs` argument.')\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n",
      "\u001b[0;32m~/Colorful_Image_Colorization/Colorization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, input, training)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# First layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 737\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    738\u001b[0m         if (any(isinstance(x, ragged_tensor.RaggedTensor) for x in input_list)\n\u001b[1;32m    739\u001b[0m             and self._supports_ragged_inputs is False):  # pylint: disable=g-bool-id-comparison\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    175\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv1_1 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [256, 256, 1]"
     ]
    }
   ],
   "source": [
    "model.build((256, 256, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"colorization\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1_1 (Conv2D)             multiple                  640       \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             multiple                  36928     \n",
      "_________________________________________________________________\n",
      "conv1_3 (Conv2D)             multiple                  36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  256       \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             multiple                  73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             multiple                  147584    \n",
      "_________________________________________________________________\n",
      "conv2_3 (Conv2D)             multiple                  147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  512       \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             multiple                  295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             multiple                  590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             multiple                  590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  1024      \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             multiple                  1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch multiple                  2048      \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch multiple                  2048      \n",
      "_________________________________________________________________\n",
      "conv6_1 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv6_2 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv6_3 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch multiple                  2048      \n",
      "_________________________________________________________________\n",
      "conv7_1 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv7_2 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "conv7_3 (Conv2D)             multiple                  2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch multiple                  2048      \n",
      "_________________________________________________________________\n",
      "upconv8 (Conv2DTranspose)    multiple                  524544    \n",
      "_________________________________________________________________\n",
      "conv8 (Conv2D)               multiple                  721465    \n",
      "=================================================================\n",
      "Total params: 30,312,889\n",
      "Trainable params: 30,307,897\n",
      "Non-trainable params: 4,992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
